- 数学基础进阶

> 矩阵对角化

$P^{-1}AP=B$

$A$为对角矩阵，$P$为单位正交矩阵$P^T=P^{-1}$

$P^T=(u_1,u_2,\cdots,u_n)$

$A=\begin{pmatrix}\lambda_1 & \\&\lambda_2\\ &&\cdots\\\end{pmatrix}$

$B=\lambda_1u_1u_1^T+\lambda_2u_2u_2^T+\cdots+\lambda_nu_nu_n^T$

> 奇异值

$\lambda_1,\lambda_2,\cdots,\lambda_n$为$A^TA$的特征值，$\vec{v_1},\vec{v_2},\cdots,\vec{v_n}$为其标准正交特征向量

$||A\vec{v_i}||^2=(A\vec{v_i})·(A\vec{v_i})=(Av_i)^T·(Av_i)=v_i^TA^TAv_i=v_i^T(A^TAv_i)=\lambda_iv_i^Tv_i=\lambda_i||\vec{v_i}||^2=\lambda_i$

$\sigma_i=\sqrt{\lambda_i}$，称$\sigma_i$为奇异值，即向量$A\vec{v_i}$的长度

> SVD分解

Singular Value Decomposition，SVD分解对任意形状适用

$A=U\Sigma V^T$

$A_{m\times n},U_{m\times m},\Sigma_{m\times n},V_{n\times n}$

$V$是$A^TA$的特征向量矩阵进行标准化

$U=(u_1,u_2,\cdots,u_m)$

前r个$u$，$\vec{u_i}=\frac{A\vec{v_i}}{\sigma_i}(\sigma_i\ne 0)$

后面的$u$使用施密特正交化拓展

$\Sigma=\begin{pmatrix}D&0\\0&0\end{pmatrix}$

$D=\begin{pmatrix}\sigma_1\\&\sigma_2 \\&&\cdots\\&&&\sigma_r\end{pmatrix}$

scipy中svd分解

```python
import numpy as np
from scipy.linalg import svd
if __name__ == "__main__":
    A = np.array([[1,2],[3,4],[5,6]])
    U, s, VT = svd(A)
    print(U)
    print(s)#返回奇异值
    print(VT)
    Sigma = np.zeros(A.shape)
    for i in range(len(s)):
        Sigma[i][i] = s[i]
    print(U.dot(Sigma).dot(VT))
```

$A^TA$为对称正定矩阵

$x^TA^TAx=(Ax)^T(Ax)\ge0\Rightarrow$正定性

$(A^TA)_{n\times n}=U^TDU$

$(AA^T)_{m\times m}=V^TDV$

$A_{m\times n}=V^T_{m\times m}\begin{pmatrix}\lambda_1^{\frac{1}{2}} \\&\lambda_2^{\frac{1}{2}}\\ &&\cdots\end{pmatrix}U_{n\times n}$

$V^T_{m\times m}=(v_1,v_2,\cdots,v_m)$

$U^T_{n\times n}=(u_1,u_2,\cdots,u_n)$

$A_{m\times n}=\lambda_1^{\frac{1}{2}}v_1u_1^T+\lambda_2^{\frac{1}{2}}v_2u_2^T+\cdots$

> 伪逆矩阵

$X_{N\times n}a_{n\times 1}=Y_{N\times1}$

一般情况下$N\ne n$

$J=min||Xa-Y||^2 \quad \frac{\partial J}{\partial a}=X^T(Xa-Y)=0$

$(X^TX)_{n\times n}$

$N>n,X^TX$一般是可逆的，$a=(X^TX)^{-1}X^TY$，若$X$可逆，$a=X^{-1}Y$

$N<n,X^TX$一定不可逆

$J=||Xa-Y||^2+\lambda||a||^2$

$ \frac{\partial J}{\partial a}=X^TXa-X^TY+\lambda a=0$

$a=(X^TX+\lambda I)^{-1}X^TY$

$a^T(X^TX+\lambda I)a=(Xa)^T(Xa)+\lambda a^T a>0 \rightarrow \lambda_i>0$

$|X^TX+\lambda I|>0$，一定可逆

> PCA

$\vec{e}=\vec{x}-p_{rj}\vec{x}=\vec{x}-(x^Tu)u$，$\vec{u}$为单位向量

$J=||\vec{e}||^2=[x-(x^Tu)u]^T[x-(x^Tu)u]=||x||^2-(x^Tu)^2$

$\max(x^Tu)^2\Leftrightarrow \max u^T(x^Tx)u$

$\max \sum_{i=1}^Nu^T(x_i^Tx_i)u=u^T\sum_{i=1}^N(x_i^Tx_i)u$

$x_i^Tx_i=X$

$\max u^TXu,st:||u||=1$

$L(u,\lambda)=u^TXu+\lambda(1-u^Tu)$

$\frac{\partial L}{\partial u}=0 \Rightarrow Xu=\lambda u$

$\frac{\partial L}{\partial \lambda}=0 \Rightarrow u^Tu=1$

> 多元函数极值

无约束条件

$\Delta=B^2-AC\begin{cases} < 0 \begin{cases} A > 0 \quad Valley\\ A < 0 \quad Peak \end{cases} \\ > 0 \quad Not \\ = 0 \quad Invalid \end{cases}$

有约束条件

$f(x,y)$在条件$\varphi(x,y)$下的极值

$F(x,y,\lambda)=f(x,y)+\lambda\varphi(x,y)$在$(x_0,y_0,\lambda_0)$

> 优化

- 梯度下降法

$f(x_1,x_2,\cdots,x_n)$

$(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\cdots,\frac{\partial f}{\partial x_n})$

$\frac{f(x_0+L\cos\theta,y_0+L\sin\theta)-f(x_0,y_0)}{L}=\sin\theta\frac{f(x_0+L\cos\theta,y_0+L\sin\theta)-f(x_0+L\cos\theta,y_0)}{L\sin\theta}+\cos\theta\frac{f(x_0+L\cos\theta,y_0)-f(x_0,y_0)}{L\sin\theta}$

$=\sin\theta·f_y(x_0,y_0)+\cos\theta·f_x(x_0,y_0)\le(\sin\theta^2+\cos\theta^2)(f_x(x_0,y_0)^2+f_y(x_0,y_0)^2)$

- 牛顿法

$y-f'(x_n)=f''(x_n)(x-x_n)$

$y=0\rightarrow x=x_n-\frac{f'(x_n)}{f''(x_n)}$

泰勒公式

$f(x)=f(x_0)+f'(x_0)(x-x_0)+\frac{f''(x_0)}{2}(x-x_0)^2+O(x)$

$f(x)=\frac{f''(x_0)}{2}x^2+(f'(x_0)-x_0f''(x_0))x+\cdots$

$-\frac{b}{2a}=0\rightarrow x=x_n-\frac{f'(x_n)}{f''(x_n)}$

- 