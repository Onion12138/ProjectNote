# 支撑向量机

## Hard Margin

直线方程$w^{T}x+b=0$

$\begin{cases} \frac{w^T x^{(i)}+b}{||w||d} \ge1 \quad \forall y^{(i)} =1 \\ \frac{w^T x^{(i)}+b}{||w||d} \le -1 \quad \forall y^{(i)} =-1 \end{cases}$

$y^{(i)}(w^Tx^{(i)}+b)\ge1$

有条件的最优化问题：max$\frac{|w^Tx+b|}{||w||}$ => min $\frac{1}{2}{||w||}^2$ (st. $y^{(i)}(w^Tx^{(i)}+b)\ge1$)

## Soft Margin

min $\frac{1}{2}{||w||}^2 + C\sum\limits_{i=1}^{m}\zeta_i$

C越大，容错空间越小

$y^{(i)}(w^Tx^{(i)}+b)\ge 1-\zeta_i \quad \zeta_i \ge 0$

## Sklearn

```python
from sklearn import datasets
X, y = datasets.make_moons(noise=0.15, random_state=666)
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import LinearSVC
from sklearn.pipeline import Pipeline
def PolynomialSVC(degree, C=1.0):
    return Pipeline([
        ("poly", PolynomialFeatures(degree=degree)),
        ("std_scaler", StandardScaler()),
        ("linerSVC", LinearSVC(C=C))
    ])
poly_svc = PolynomialSVC(degree=3)
poly_svc.fit(X, y)
```

使用核函数

```python
from sklearn.svm import SVC
def PolynomialKernelSVC(degree, C=1.0):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("kernelSVC", SVC(kernel="poly", degree=degree, C=C))
    ])
```

多项式核函数$K(x,y) =(x·y+c)^d$

线性核函数$K(x,y)=x·y$

## 高斯核函数

$K(x,y)=e^{-\gamma{||x-y||}^2}$

高斯核：对于每一个数据点都是landmark

$m*n$的数据映射成了$m*m$的数据

多项式特征：依靠升维使原本线性不可分的点线性可分

```python
from sklearn.svm import SVC
def RBFKernelSVC(gamma=1.0):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("svc", SVC(kernel="rbf", gamma=gamma))
    ])
```

gamma较大：过拟合

gamma较小：欠拟合

## SVM解决回归问题

```python
from sklearn.svm import LinearSVR
from sklearn.svm import SVR
def StandardLinerSVR(epsilon=0.1):
    return Pipeline([
        ("std_scaler", StandardScaler()),
        ("linearSVR", LinearSVR(epsilon=epsilon))
    ])
```

